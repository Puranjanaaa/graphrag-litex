# ğŸ“˜ GraphRAG-LiteX: Lightweight Graph-Based Retrieval-Augmented Generation

![GraphRAG-LiteX Banner](https://user-images.githubusercontent.com/12345678/graphrag_litex_banner.png)

GraphRAG-LiteX is a **lightweight**, **modular**, and **locally executable** version of the GraphRAG framework, built to run on modern LLMs like DeepSeek via LM Studio. It enables **knowledge graph-based retrieval-augmented generation** and provides a built-in evaluation framework comparing GraphRAG against traditional vector-based RAG pipelines.

---

## ğŸŒŸ Key Features

- ğŸ§  Graph-based RAG: Combines claims and entity extraction into a dynamic knowledge graph.
- ğŸ”„ Async LLM integration: Optimized for high-throughput local processing with LM Studio.
- ğŸ§ª Evaluation-ready: Includes a comprehensive side-by-side evaluation with VectorRAG.
- âš™ï¸ Modular: Easily extend components like extraction prompts or graph logic.
- ğŸ’¡ Insightful: Evaluates outputs based on global coherence, faithfulness, and empowerment.

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ config.py
â”œâ”€â”€ graphrag_lite_x.py (Main pipeline - graph builder + answer generator)
â”œâ”€â”€ main.py (Optional entry point)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env (API keys and model info)
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ async_utils.py
â”‚ â”œâ”€â”€ io_utils.py
â”‚ â”œâ”€â”€ prompts.py
â”‚ â””â”€â”€ llm_client.py
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ claim.py
â”‚ â”œâ”€â”€ entity.py
â”‚ â”œâ”€â”€ relationship.py
â”‚ â””â”€â”€ knowledge_graph.py
â”‚
â””â”€â”€ evaluation/
â”œâ”€â”€ evaluate_graphrag.py (Compares GraphRAG-LiteX vs VectorRAG)
â”œâ”€â”€ llm_judge.py (LLM-based evaluation)
â””â”€â”€ *.csv (Saved evaluation results)


---

## ğŸ§° Requirements

- Python 3.9+
- LM Studio (for local LLM inference)
- DeepSeek-compatible model (e.g. `deepseek-r1-distill-qwen-7b`)

### ğŸ”§ Setup

```bash
pip install -r requirements.txt
Add a .env file:

LLM_API_KEY=your_api_key_here
LLM_MODEL_NAME=deepseek-r1-distill-qwen-7b
```
### ğŸš€ Running the Pipeline

Run GraphRAG-LiteX:
```bash
python3 graphrag_lite_x.py --documents ./data/documents
```

Evaluate GraphRAG-LiteX vs VectorRAG:
```bash
python3 -m evaluation.evaluate_graphrag
```

### ğŸ–¼ï¸ System Architecture

Documents â†’ Claim & Entity Extraction
Nodes/Edges â†’ Knowledge Graph
Query â†’ Subgraph Retrieval
Subgraph â†’ LLM â†’ Final Answer

### ğŸ“Š Sample Evaluation Output

System	Wins	Percentage
GraphRAG-LiteX	10	83.3%
VectorRAG	2	16.7%

ğŸ“ˆ Based on criteria like comprehensiveness, faithfulness, empowerment, and coherence.

### ğŸ§  Model Recommendation

Tested and tuned with:

âœ… deepseek-r1-distill-qwen-7b
âœ… LM Studio for inference
âœ… Works with any OpenAI-compatible endpoint

### ğŸ“Œ Use Cases

âœ… Research on graph-based reasoning
âœ… Comparative retrieval models
âœ… NLP and QA prototyping
âœ… Lightweight RAG system demos
ğŸ¤ Contributing

We welcome PRs, issues, and suggestions!

Please:

Follow the folder structure
Include docstrings and inline comments
Add test cases for new components (if applicable)

### ğŸ™Œ Credits

Inspired by Microsoft Researchâ€™s GraphRAG methodology for global query-focused summarization:
ğŸ“„ From Local to Global: A Graph RAG Approach to Query-Focused Summarization
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson
arXiv:2404.16130
DOI: 10.48550/arXiv.2404.16130

Uses DeepSeekâ€™s distill model (deepseek-r1-distill-qwen-7b) for local LLM inference:
ğŸ”— Hugging Face model card:
https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
Powered locally using LM Studio â€“ open-source desktop LLM inference engine:
ğŸŒ Official website:
https://lmstudio.ai

Built for experimental research and educational use.

 ### ğŸ“« Contact

puranja@gmail.com for questions or collaboration.

